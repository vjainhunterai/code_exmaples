import boto3
from datetime import datetime

def list_s3_files(bucket_name, folder_name, aws_access_key_id, aws_secret_access_key, region_name):
    # Create an S3 client with explicit credentials
    s3_client = boto3.client(
        's3',
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        region_name=region_name
    )
    
    # Paginator to handle large number of files
    paginator = s3_client.get_paginator('list_objects_v2')

    # Paginate over the objects in the specified folder (prefix)
    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=folder_name)

    found_files = False

    for page in page_iterator:
        # If no files found, handle it
        if 'Contents' not in page:
            continue  # Continue in case there are paginated results

        for obj in page['Contents']:
            found_files = True  # At least one file was found
            file_name = obj['Key']
            file_size = obj['Size']
            upload_time = obj['LastModified']

            # Fetch object metadata to get user if it's stored
            metadata = s3_client.head_object(Bucket=bucket_name, Key=file_name).get('Metadata', {})
            user = metadata.get('user', 'Unknown')

            # Convert upload time to a readable format
            upload_time_str = upload_time.strftime("%Y-%m-%d %H:%M:%S")

            print(f"File: {file_name}, Size: {file_size} bytes, Uploaded on: {upload_time_str}, User: {user}")

    if not found_files:
        print(f"No files found in folder '{folder_name}' in bucket '{bucket_name}'")

if __name__ == "__main__":
    bucket_name = input("Enter the S3 bucket name: ")
    folder_name = input("Enter the folder name (prefix) in the bucket: ")

    # Credentials (replace with your actual credentials)
    aws_access_key_id = 'your-access-key-id'
    aws_secret_access_key = 'your-secret-access-key'
    region_name = 'your-region'  # e.g., 'us-west-2'

    # Add debug message to see the prefix being used
    print(f"Looking for files in the bucket '{bucket_name}' under folder '{folder_name}'...")

    list_s3_files(bucket_name, folder_name, aws_access_key_id, aws_secret_access_key, region_name)
