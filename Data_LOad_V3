import os
import pandas as pd
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from datetime import datetime
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import hmac

class DataLoader:
    def __init__(self, file_type, table_name, directory_path, delimiter, db_uri, load_user):
        self.file_type = file_type.lower()
        self.table_name = table_name
        self.directory_path = directory_path
        self.delimiter = delimiter
        self.db_uri = db_uri
        self.load_user = load_user
        self.engine = create_engine(db_uri)
        self.Session = sessionmaker(bind=self.engine)
    
    def load_data(self):
        files = [f for f in os.listdir(self.directory_path) if self.file_type in f.lower()]
        total_rows = 0
        for file in files:
            file_path = os.path.join(self.directory_path, file)
            data = self._read_file(file_path)
            if data is not None:
                data = self._add_hash_key(data)
                data = self._add_metadata(data)
                rows_inserted = self._insert_data(data)
                total_rows += rows_inserted
        return total_rows
    
    def _read_file(self, file_path):
        try:
            return pd.read_csv(file_path, delimiter=self.delimiter)
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            return None

    def _add_hash_key(self, data):
        def hash_row(row):
            concatenated = ''.join(str(row[col]) for col in data.columns)
            digest = hmac.HMAC(b'secret_key', hashes.SHA256(), backend=default_backend())
            digest.update(concatenated.encode('utf-8'))
            return digest.finalize().hex()
        
        data.insert(0, 'hash_key', data.apply(hash_row, axis=1))
        return data
    
    def _add_metadata(self, data):
        data['load_user'] = self.load_user
        data['load_date'] = datetime.now()
        return data

    def _insert_data(self, data):
        try:
            with self.engine.begin() as connection:
                data.to_sql(self.table_name, con=connection, if_exists='append', index=False)
                result = connection.execute(text(f"SELECT COUNT(*) FROM {self.table_name}"))
                return result.scalar()
        except Exception as e:
            print(f"Error inserting data into {self.table_name}: {e}")
            return 0
    
    def close(self):
        self.engine.dispose()


###############################3


from datetime import datetime

db_uri = "mysql+pymysql://admin:Gpoproddb!#!@prod-db.c969yoyq9cyy.us-east-1.rds.amazonaws.com/joblog_metadata"
file_type = 'AP'
table_name = 'target_table'
directory_path = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\processed\\processed_valid\\AP'
delimiter = '|'
load_user = 'admin'

loader = DataLoader(file_type=file_type, table_name=table_name, directory_path=directory_path, delimiter=delimiter, db_uri=db_uri, load_user=load_user)
processed_cnt = loader.load_data()
loader.close()

end_time = datetime.now()
db_handler = databaseHandler(db_uri)
audit_logger = auditLogger(db_handler)

# Update the audit entry with the processed count
updated_values = {
    'job_status': 'Completed',
    'processed_cnt': processed_cnt,
    'rejection_rsn': invalid_rows_count  # Assuming `invalid_rows_count` is calculated elsewhere
}
audit_logger.updateAudit(exec_id, updated_values)

