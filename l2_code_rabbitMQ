import os
import pika
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# DBHandler Class to manage database connections and insert audit entries
class DBHandler:
    def __init__(self, db_url):
        self.engine = create_engine(db_url)
        self.Session = sessionmaker(bind=self.engine)
        self.session = self.Session()

    def insertL2AuditEntry(self, audit_entry):
        try:
            insert_stmt = text("""
                INSERT INTO l2_audit_data (
                    Job_ID, Job_Name, Customer_Name, Flow_Type, Operation_type, Source_Count, Target_count,
                    New_rcrd_count, update_rcrd_count, Start_Time, End_Time, Status, Insert_By, Error_desc,
                    L1_load_date, L2_load_date
                ) VALUES (
                    :Job_ID, :Job_Name, :Customer_Name, :Flow_Type, :Operation_type, :Source_Count, :Target_count,
                    :New_rcrd_count, :update_rcrd_count, :Start_Time, :End_Time, :Status, :Insert_By, :Error_desc,
                    :L1_load_date, :L2_load_date
                )
            """)
            self.session.execute(insert_stmt, audit_entry)
            self.session.commit()
        except Exception as e:
            print(f"Error inserting audit entry: {e}")
            self.session.rollback()

    def close(self):
        self.session.close()

# RabbitMQ Helper to publish messages to RabbitMQ queue
def publish_to_rabbitmq(queue_name, message):
    try:
        credentials = pika.PlainCredentials('admin', 'admin')
        connection_params = pika.ConnectionParameters(
    '172.31.30.137',  # Replace with 'your-server-ip' if connecting remotely
    5672,
    '/',  # The default vhost
    credentials
)
        connection = pika.BlockingConnection(connection_params)
        channel = connection.channel()

        # Declare a queue
        channel.queue_declare(queue=queue_name)

        # Publish message to the queue
        channel.basic_publish(exchange='',
                              routing_key=queue_name,
                              body=message)
        print(f" [x] Sent '{message}' to {queue_name} queue")
        connection.close()
    except Exception as e:
        print(f"Error publishing to RabbitMQ: {e}")

# Helper function to create DB connection using DBHandler
def create_connection(db_url):
    return DBHandler(db_url)

# Placeholder function to simulate SQL execution
def execute_sql(session, query, variables):
    try:
        session.execute(text(query), variables)
        return "SUCCESS", ""
    except Exception as e:
        return "FAIL", str(e)

# Placeholder function to simulate l2Audit count logic
def l2Audit_cnt(srcDb, targetDb, l1InvTable, targetTable, customer_name, L1_load_date, engine):
    # Perform source and target count logic, here we return dummy counts
    return 100, 98  # Dummy counts for testing

# Simulate fetching SQL queries from metadata
def fetch_all_sql_queries(session, query_meta_table, ctxArea):
    return [("SELECT * FROM your_table WHERE customer_name = :context_customer",)]  # Example query list

# Placeholder functions to read L1 and L2 audit tables (to be replaced by actual logic)
def read_l1_inv_table(l1InvTable, l1db_name, engine):
    # Return dummy data simulating dataframe output
    return [{'Customer_Name': 'TestCustomer', 'L1_load_date': '2023-09-20'}]

def read_l2_audit_table(l2AuditTable, l2_auditdb, engine):
    # Return dummy data simulating dataframe output
    return [{'Customer_Name': 'TestCustomer', 'L1_load_date': '2023-09-20'}]

# Simulate delta calculation between L1 and L2 tables
def delta_cust_details(df1, df2):
    # Return dummy data for delta customers
    return [{'Customer_Name': 'TestCustomer', 'L1_load_date': '2023-09-20'}]

# Main L2 Execution logic integrating DBHandler and RabbitMQ
def l2Execution():
    try:
        # Initialize RabbitMQ queue name
        queue_name = "l2_audit_updates"

        # Initialize DB connection
        dbUrl = 'mysql+mysqlconnector://admin:Gpoproddb!#!@prod-db.c969yoyq9cyy.us-east-1.rds.amazonaws.com/joblog_metadata'
        db_handler = create_connection(dbUrl)

        # Simulate reading metadata and context from the DB
        l2MetadataTable = "prod_context_l2_l3"
        meta_db_name = "joblog_metadata"

        # Metadata
        meta_l2_keys = {
            "L1_Inv_Table": "l1_invoice_table",
            "L2_Audit_Table": "l2_audit_data",
            "L1_server_DB": "db_l1",
            "L2_Audit_Db": "db_l2",
            "L2_Server_Database": "db_l2",
            "L2_Inv_Table": "l2_invoice_table",
            "L2_ctxarea": "l2_context_area",
            "L2_Query_Metadata": "l2_query_metadata"
        }

        # Get metadata details
        l1InvTable = meta_l2_keys["L1_Inv_Table"]
        l2AuditTable = meta_l2_keys["L2_Audit_Table"]
        query_meta_table = meta_l2_keys["L2_Query_Metadata"]
        ctxArea = meta_l2_keys["L2_ctxarea"]

        # Fetch all SQL queries
        sql_queries = fetch_all_sql_queries(db_handler.session, query_meta_table, ctxArea)

        # Read data from L1 and L2 tables
        df1 = read_l1_inv_table(l1InvTable, meta_l2_keys["L1_server_DB"], db_handler.engine)
        df2 = read_l2_audit_table(l2AuditTable, meta_l2_keys["L2_Audit_Db"], db_handler.engine)

        # Calculate delta between L1 and L2 tables
        df_delta = delta_cust_details(df1, df2)

        # Loop through each delta row and execute SQL
        for delta in df_delta:
            variables = {
                "context_customer": delta['Customer_Name'],
                "context_l1_date": delta['L1_load_date']
            }
            customer_name = variables['context_customer']
            L1_load_date = variables['context_l1_date']

            # Execute SQL queries for each customer
            for sql_query in sql_queries:
                sql_query_text = sql_query[0]
                status, error_message = execute_sql(db_handler.session, sql_query_text, variables)

                # Fetch source and target counts
                Source_Count, Target_count = l2Audit_cnt(
                    meta_l2_keys["L1_server_DB"], 
                    meta_l2_keys["L2_Server_Database"], 
                    l1InvTable, 
                    meta_l2_keys["L2_Inv_Table"], 
                    customer_name, 
                    L1_load_date, 
                    db_handler.engine
                )

                # Determine the job status
                if status == "FAIL":
                    job_status = "FAILED"
                else:
                    job_status = "SUCCESS"

                # Prepare the audit entry
                audit_entry = {
                    'Job_ID': "1234",
                    'Job_Name': "L2_Load",
                    'Customer_Name': customer_name,
                    'Flow_Type': "Direct",
                    'Operation_type': "L2_Audit",
                    'Source_Count': int(Source_Count),
                    'Target_count': int(Target_count),
                    'New_rcrd_count': 0,
                    'update_rcrd_count': 0,
                    'Start_Time': datetime.now(),
                    'End_Time': datetime.now(),
                    'Status': job_status,
                    'Insert_By': os.getlogin(),
                    'Error_desc': error_message,
                    'L1_load_date': L1_load_date,
                    'L2_load_date': datetime.now()
                }

                # Insert audit entry into the database
                db_handler.insertL2AuditEntry(audit_entry)

                # Publish the audit entry to RabbitMQ
                publish_to_rabbitmq(queue_name, str(audit_entry))

                # Stop further execution if SQL query failed
                if status == "FAIL":
                    raise Exception(f"SQL Execution Failed: {error_message}")

        # Close the DB connection
        db_handler.close()

    except Exception as e:
        print(f"Error in main execution: {e}")

# Execute the L2 process
l2Execution()
