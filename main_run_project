import boto3
from botocore.client import BaseClient
import pandas as pd
from cryptography.fernet import Fernet
import os
import uuid
from sqlalchemy import create_engine, Table, Column, Integer, String, MetaData, DateTime, insert, select
from sqlalchemy.orm import sessionmaker

from src.utils import (
    readEncryptedConfig,
    readMetadata,
    decryptData,
    listFilesInS3,
    createMysqlConnection,
    executeQuery,
    databaseHandler,
    stepLogger,
    auditLogger,
    readDocumentsFromPath,
    find_delimiter,
    schemaValidator,
)
from src.etl import (
    getS3Directories,
    listFilesInS3Directories,
    getIncrementalFiles,
    insertFilesToDb,
    fetchUnprocessedFiles,
    downloadAndProcessFile,
    updateFileFlag
)

from src.data_quality import (
    fileSizeProcessor
)

if __name__ == "__main__":
    # read ecrypted data and get decrypt database configuration
    relativeExcelPath = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\metadata\\paths.xlsx'
    config = readEncryptedConfig(relativeExcelPath)

    # read Metadata from DataBase
    print(config)
    (
        dbConnection,
        session,
        s3FilesTable,
        awsAccessKeyId,
        awsSecretAccessKey
    ) = readMetadata(config)
    dbConnection = dbConnection
    session = session
    s3FilesTable = s3FilesTable
    awsAccessKeyId = awsAccessKeyId
    awsSecretAccessKey = awsSecretAccessKey
    regionName = 'us-east-1'  # these need to be available in database
    bucketName = 'etlhunter'  # these need to be available in database
    daysS3 = 25                # these need to be available in database
   # create s3 client
    s3Client: BaseClient = boto3.client('s3',
                                        aws_access_key_id=awsAccessKeyId,
                                        aws_secret_access_key=awsSecretAccessKey,
                                        region_name=regionName)

    # getS3Directories
    listDir = getS3Directories(daysS3)
    print(listDir)
    # list all files available in directory
    s3Files = listFilesInS3Directories(listDir, s3Client, bucketName)
    #print(s3Files)

    # get all file names from audit tale
    auditFiles_query = ('select distinct file_name,s3_last_modified_date from joblog_metadata.L1_audit_data')
    auditFiles = pd.read_sql(auditFiles_query, dbConnection)

    # Get only incremental files
    incrementalFilesDf = getIncrementalFiles(s3Files, auditFiles)
    #print(incrementalFilesDf)

    # Insert increment files in data
    insertFilesToDb(incrementalFilesDf, s3FilesTable, session, insert)

    # fetchUnprocessedFile
    fetchUnprocessedFile = fetchUnprocessedFiles(s3FilesTable, session, select)
    #print(fetchUnprocessedFile)

    #  download all files from S3 to local

    localDirectoryLA = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\raw\\LA'


    processed_files = set()

    if isinstance(listDir, list):
        for directory in listDir:
            for file in fetchUnprocessedFile:

                file_identifier = file['File_Name']  # Adjust this key based on your actual file structure

                if file_identifier not in processed_files:
                    try:
                        downloadAndProcessFile(s3Client, bucketName, file, directory, localDirectoryLA)
                        processed_files.add(file_identifier)
                    except Exception as e:
                        print(f"An error occurred: {e}")
    # file size check
    filePath = "C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\raw\\LA\\Hurley Medical 8.2021-7.2023.csv"  # Replace with your file path
    
    processor = fileSizeProcessor(filePath)
    result = processor.processFile()
    print(result)

