import os
import shutil
import pandas as pd
from sqlalchemy import create_engine, MetaData, Table
from sqlalchemy.orm import sessionmaker
from datetime import datetime

class schemaValidator:
    def __init__(self, dbUrl):
        self.engine = create_engine(dbUrl)
        self.Session = sessionmaker(bind=self.engine)
        self.metadata = MetaData()

    def getSchemaFromDb(self, schemaTable, tableName):
        session = self.Session()
        try:
            schema_table = Table(schemaTable, self.metadata, autoload_with=self.engine)
            query = session.query(schema_table.c.Column_name).filter_by(table_name=tableName)
            df_schema = pd.read_sql(query.statement, session.bind)
            if not df_schema.empty:
                schema = df_schema.iloc[0]['Column_name'].split('|')
            else:
                schema = None
            return schema
        except Exception as e:
            print(f"Error fetching schema from database: {e}")
            return None
        finally:
            session.close()

    def getFileHeader(self, filePath, delimiter):
        try:
            df = pd.read_csv(filePath, delimiter=delimiter, nrows=0)
            return df.columns.tolist()
        except Exception as e:
            print(f"Error reading file header: {e}")
            return []

    def compareSchemaAndHeader(self, schema, header):
        try:
            if schema == header:
                return True, "Schema matches the file header."
            else:
                schema_set = set(schema)
                header_set = set(header)
                missing_columns = schema_set - header_set
                extra_columns = header_set - schema_set
                report = "Schema does not match the file header.\n"
                if missing_columns:
                    report += f"Missing columns: {', '.join(missing_columns)}\n"
                if extra_columns:
                    report += f"Extra columns: {', '.join(extra_columns)}\n"
                return False, report
        except Exception as e:
            print(f"Error comparing schema and header: {e}")
            return False, "Error during schema comparison."

    def getSchemaMapping(self, mappingTable, fileType):
        session = self.Session()
        try:
            mapping_table = Table(mappingTable, self.metadata, autoload_with=self.engine)
            query = session.query(mapping_table.c.source_column, mapping_table.c.target_column).filter_by(file_type=fileType)
            df_mapping = pd.read_sql(query.statement, session.bind)
            if not df_mapping.empty:
                schema_mapping = dict(zip(df_mapping['source_column'], df_mapping['target_column']))
            else:
                schema_mapping = None
            return schema_mapping
        except Exception as e:
            print(f"Error fetching schema mapping from database: {e}")
            return None
        finally:
            session.close()

    def applySchemaMapping(self, df, schemaMapping):
        try:
            df = df.rename(columns=schemaMapping)
            return df
        except Exception as e:
            print(f"Error applying schema mapping: {e}")
            return df

    def validate(self, schemaTable, tableName, filePath, delimiter, mappingTable, fileType, outputFilePath):
        try:
            schema = self.getSchemaFromDb(schemaTable, tableName)
            if not schema:
                return False, "No schema found for table '{}'.".format(tableName)

            header = self.getFileHeader(filePath, delimiter)
            if not header:
                return False, "Failed to read the file header."

            is_match, report = self.compareSchemaAndHeader(schema, header)
            if not is_match:
                return False, report

            # Skip reading the full file to avoid tokenizing errors
            # Only proceed with the mapping if header matches
            session = self.Session()
            try:
                mapping_table = Table(mappingTable, self.metadata, autoload_with=self.engine)
                query = session.query(mapping_table.c.target_column).filter_by(file_type=fileType)
                df_mapping = pd.read_sql(query.statement, session.bind)
                if not df_mapping.empty:
                    columnNames = df_mapping['target_column'].tolist()
                else:
                    return False, "No mappings found for file type '{}'.".format(fileType)

                # Only rename columns based on the header
                source_to_target_mapping = {source: target for source, target in zip(header, columnNames)}

                # Write an empty DataFrame with the mapped columns to the output file
                pd.DataFrame(columns=columnNames).to_csv(outputFilePath, sep=delimiter, index=False)

                return True, "Schema mapping applied. Empty file with correct header saved to '{}'.".format(outputFilePath), 0

            except Exception as e:
                print(f"Error during schema mapping: {e}")
                return False, "Error during schema mapping.", 0
            finally:
                session.close()

        except Exception as e:
            print(f"Error in validation process: {e}")
            return False, "Validation process encountered an error.", 0

    def validate_directory(self, schemaTable, tableName, directory, delimiter, mappingTable, fileType, targetDirectory, rejectionDirectory):
        os.makedirs(targetDirectory, exist_ok=True)
        os.makedirs(rejectionDirectory, exist_ok=True)

        results = []
        total_source_record_count = 0
        total_target_record_count = 0

        for filename in os.listdir(directory):
            filePath = os.path.join(directory, filename)

            if os.path.isfile(filePath):
                outputFilePath = os.path.join(targetDirectory, filename)

                try:
                    success, report, record_count = self.validate(schemaTable, tableName, filePath, delimiter, mappingTable, fileType, outputFilePath)

                    if success:
                        results.append((filename, 1, record_count))  # 1 indicates success
                    else:
                        rejectionFilePath = os.path.join(rejectionDirectory, filename)
                        shutil.copy(filePath, rejectionFilePath)
                        results.append((filename, 0, 0))  # 0 indicates failure

                except Exception as e:
                    results.append((filename, -1, 0))  # -1 indicates error
                    print(f"Error processing file '{filename}': {e}")
            else:
                results.append((filename, -2, 0))  # -2 indicates non-file

        return results, total_source_record_count, total_target_record_count





===========================


# Validate schema for each file
    dbUrl1 = "mysql+pymysql://admin:Gpoproddb!#!@prod-db.c969yoyq9cyy.us-east-1.rds.amazonaws.com/joblog_metadata"
    tableName = 'AP'
    schemaTable = 'Table_Schema'
    mappingTable = 'mapping_table'
    fileType = 'AP'
    directory_path = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\processed\\processed_source\\AP'
    target_directory = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\processed\\processed_schema_valid\\AP'
    rejection_directory = 'C:\\Users\\jvineet\\PycharmProjects\\PythonLearnings\\Data\\processed\\processed_rejection\\AP'


    schema_validator = schemaValidator(dbUrl1)
    start_time = datetime.now()
    results, total_source_record_count, total_target_record_count = schema_validator.validate_directory(
        schemaTable, tableName, directory_path, delimiter, mappingTable, fileType, target_directory, rejection_directory
    )
    end_time = datetime.now()






Delimiter for C:\Users\jvineet\PycharmProjects\PythonLearnings\Data\raw\LA\Community_Hospital_and_Wellness_110923_ap.txt is '|'
Moved 'Community_Hospital_and_Wellness_110923_ap.txt' to 'C:\Users\jvineet\PycharmProjects\PythonLearnings\Data\processed\processed_source\AP\Community_Hospital_and_Wellness_110923_ap.txt'
Error processing file 'Community_Hospital_and_Wellness_110923_ap.txt': not enough values to unpack (expected 3, got 2)


