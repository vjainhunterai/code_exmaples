pip install transformers torch


from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load the pre-trained BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Load the pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


def preprocess_text(text, tokenizer):
    # Tokenize the text
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,  # Add [CLS] and [SEP] tokens
        max_length=64,            # Maximum length of the sequence
        truncation=True,          # Truncate if longer than max_length
        padding='max_length',     # Pad shorter sequences to max_length
        return_tensors='pt',      # Return PyTorch tensors
    )
    return encoding['input_ids'], encoding['attention_mask']

# Example text to classify
review = "The movie was absolutely amazing!"

# Preprocess the text
input_ids, attention_mask = preprocess_text(review, tokenizer)




# Put the model in evaluation mode
model.eval()

# Perform inference (no need for gradient computation)
with torch.no_grad():
    outputs = model(input_ids, attention_mask=attention_mask)
    logits = outputs.logits

# Apply softmax to get probabilities
probs = torch.nn.functional.softmax(logits, dim=1)

# Get the predicted label (0 = negative, 1 = positive)
predicted_label = torch.argmax(probs, dim=1).item()

# Interpret the prediction
label_map = {0: 'negative', 1: 'positive'}
print(f"Review: '{review}' -> Predicted as: {label_map[predicted_label]}")


Review: 'The movie was absolutely amazing!' -> Predicted as: positive
